# -*- coding: utf-8 -*-
"""HR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ROuGFWcR-bq-tdCEGFrBbqq75Z5YRw3I
"""

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

"""DATA COLLECTION"""

train = pd.read_csv("/content/train_LZdllcl.csv")

train.head()

train.shape

train.info()

# Checking missing values
print("Missing values:\n", train.isnull().sum())

# Data types
print(train.dtypes)

# Summary statistics
print(train.describe())

"""DATA VISUALIZATION"""

# How balanced is the target variable?
sns.countplot(x='is_promoted', data=train)
plt.title('Promotion Distribution')
plt.xlabel('Is Promoted')
plt.ylabel('Count')
plt.show()

plt.figure(figsize=(10,6))
sns.histplot(train['age'], bins=30, kde=True)
plt.title('Age Distribution')
plt.xlabel('Age')
plt.ylabel('Number of Employees')
plt.show()

plt.figure(figsize=(12,6))
sns.barplot(x='department', y='is_promoted', data=train)
plt.title('Promotion Rate by Department')
plt.xticks(rotation=45)
plt.ylabel('Promotion Rate')
plt.show()

# Correlation heatmap (only numeric features)
plt.figure(figsize=(12,8))
# Select only numeric features for correlation calculation
numeric_features = train.select_dtypes(include=np.number)
sns.heatmap(numeric_features.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Feature Correlation Heatmap')
plt.show()

"""PREPROCESSING"""

# Save employee_id separately for final submission
train_employee_id = train['employee_id']

# Drop 'employee_id' (not useful for prediction)
train.drop('employee_id', axis=1, inplace=True)

# Handling missing values in 'previous_year_rating' column
train['previous_year_rating'].fillna(train['previous_year_rating'].mode()[0], inplace=True)
# Access the first element of mode for consistent replacement

train['education'].fillna(train['education'].mode()[0], inplace=True)

# Checking missing values
print("Missing values:\n", train.isnull().sum())

num_cols = ['age','length_of_service','avg_training_score']

#feature scaling using standardization
from sklearn.preprocessing import StandardScaler
ss = StandardScaler()

for cols in num_cols :
   df[cols] = ss.fit_transform(df[[cols]])

# Check categorical feature values
cat_cols = ['department', 'region', 'education', 'gender', 'recruitment_channel']
for col in cat_cols:
    print(f"{col}: {train[col].unique()}")
    print(f"{col}: {test[col].unique()}")
    print("-"*50)

train.columns

# to label the target variable we use label encoder
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()

train["is_promoted"] = le.fit_transform(train['is_promoted'])

for column in cat_cols:
    train[column] = le.fit_transform(train[column])

train

"""MOSEL BUILDING"""

from sklearn.model_selection import train_test_split as tts

#Splitting the dataset
x = train.drop('is_promoted', axis=1) #Features
y = train['is_promoted'] #Target variable

#Train test split(80% train , 20% split)
x_train, x_test, y_train, y_test = tts(x,y,test_size=0.2, random_state=28)

from sklearn.metrics import classification_report, accuracy_score

#model building using Random Forest
from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier()
rf_model.fit(x_train,y_train)


#prediction
y_pred = rf_model.predict(x_test)


# Evaluate the model
print("Classification Report:")
print(classification_report(y_test, y_pred))
accuracy_rf = accuracy_score(y_test,y_pred)
print("Decision Tree Accuracy :" , accuracy_rf)

#model building using knn
from sklearn.neighbors import KNeighborsClassifier

knn_model = KNeighborsClassifier(n_neighbors=3)
knn_model.fit(x_train,y_train)


#prediction
y_pred = knn_model.predict(x_test)

#Evaluation
accuracy_knn = accuracy_score(y_test,y_pred)
print("Decision Tree Accuracy :" , accuracy_knn)

#model building using Naive Bayes

from sklearn.naive_bayes import GaussianNB
from sklearn.impute import SimpleImputer # Import SimpleImputer

nb_model = GaussianNB()
nb_model.fit(x_train,y_train)


#prediction
y_pred = nb_model.predict(x_test)

#Evaluation
print("Classification Report:")
print(classification_report(y_test, y_pred))
accuracy_nb = accuracy_score(y_test,y_pred)
print("Decision Tree Accuracy :" , accuracy_nb)

#XGBoost
from xgboost import XGBClassifier
# Initialize the XGBClassifier
xgb_classifier = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)

# Train the model
xgb_classifier.fit(x_train, y_train)

# Make predictions
y_pred = xgb_classifier.predict(x_test)

# Evaluate the model
print("Classification Report:")
print(classification_report(y_test, y_pred))
print("Accuracy:", accuracy_score(y_test, y_pred))

#svm
#model building using SVM
from sklearn.svm import SVC

svm_model = SVC()
svm_model.fit(x_train,y_train)


#prediction
y_pred = svm_model.predict(x_test)

#Evaluation
accuracy_svm = accuracy_score(y_test,y_pred)
print("SVM Accuracy :" , accuracy_svm)

"""Prediction using test data"""

test = pd.read_csv("/content/test_2umaH9m.csv")

# Checking missing values
print("Missing values:\n", test.isnull().sum())

# Handling missing values in 'previous_year_rating' column
test['previous_year_rating'].fillna(test['previous_year_rating'].mode()[0], inplace=True)

test['education'].fillna(test['education'].mode()[0], inplace=True)

print("Missing values:\n", test.isnull().sum())

# encoding data for traing ml model
label_encoder = LabelEncoder()

for column in cat_cols:
    test[column] = label_encoder.fit_transform(test[column])

test_employee_id = test['employee_id']

test.drop('employee_id', axis=1, inplace=True)

test.columns

final_result= xgb_classifier.predict(test)

final_result

len(final_result)

sample_sub = pd.read_csv("/content/sample_submission_M0L0uXE.csv")

sample_sub

sample_sub['is_promoted'] = final_result

sample_sub

sample_sub.to_csv("Result.csv", index=False)

sample_sub.head()